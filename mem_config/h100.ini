; This config file its for Tesla H100-SXM5-80GB
[Default]
# For this H100, there are 132 SMs with 2 SMs per TPC, so 66 TPCs.
# Each SM in H100 has 256 kB L1 instead of 192 as in A100
# 1/(132 * 256)
L1_THROUGHPUT_FIX = 0.0000295928
#L1_THROUGHPUT_PEAK = 132 * 256
L1_THROUGHPUT_PEAK = 33792
# For H100, there are 66 TPCs and each one has 2 uTLB (?).
# uTLB_THROUGHPUT_FIX = 1/( 66 * 2 )
# https://images.anandtech.com/doci/14255/TU117_Unofficial_1650_575px.png
uTLB_THROUGHPUT_FIX = 0.00757575757
# For H100, there are 8 GPCs and each one has 2 L1 TLBs (?).
# L1_TLB_THROUGHPUT_FIX = 1 / (8 * 2)
L1_TLB_THROUGHPUT_FIX = 0.0625
# For H100, there are 100 L2 slices (Source D, times 1.25 for H100 L2 cache size) and each read on one slice is 64 bytes. (?)
# Assuming every L2 slice is 512 KB.
# L2_THROUGHPUT_FIX = 1/ ( 100 * 64)
L2_THROUGHPUT_FIX = 0.00015625
# FB (frame buffer) is the same as DRAM, HBM2
# Peak H100 DRAM bandwidth is 3352 GB/s, and boost clock is 1.98 GHz (Source C)
# FB_THROUGHPUT_FIX = 1 / 3352  * 1.98
FB_THROUGHPUT_FIX = 0.00059069212

compute_capability=90

# Josh: these are unchanged from A100, per Source C
max_avtive_warps_per_SM = 64
warp_size = 32
quadrants_per_SM = 4

# The following two settings are used to limit the number of nodes in decision trees
max_number_of_showed_nodes = 5
max_percentage_of_showed_nodes = 0.99
BYTES_PER_L2_INSTRUCTION = 32
BYTES_PER_L1_INSTRUCTION = 128
conflict_high_threshold = 0.1
low_activewarps_per_activecycle = 16
high_l1_throughput = 0.8
high_l1_hit_rate = 0
high_l1_conflict_rate = 0.2
low_access_per_activate = 4
low_bank_per_access = 10

# @todo
within_load_coalescing_ratio = 1
low_l1_hit_rate = 0.75
high_utlb_miss_rate = .25
high_l2_miss_rate = 0.25
high_l2_bank_conflict_rate = 0.2
high_not_predicated_off_thread_per_inst_executed = 17
max_not_predicated_off_thread_per_inst_executed = 32
low_compress_rate = .05

# Latency part
# Josh: source B
L1_LATENCY_FIX = 23
uTLB_LATENCY_FIX = 1
l1TLB_LATENCY_FIX = 10
# Josh: source B
l2_latency = 160 # using near hit latency
# L2 latencies are bimodally distributed in Hopper due to bifurcation of L2 cache
fb_latency = 379
# Josh: source B

# limit how many instructions showed for stall reasons
max_percentage_of_showed_source_code_nodes = 0.95
max_number_of_showed_source_code_nodes = 10

# Josh: key to sources
# Source A is the Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis arXiv paper from 2025
# Source B is Chester Lam's Chips and Cheese article "Nvidiaâ€™s H100: Funny L2, and Tons of Bandwidth"
#   Link: https://chipsandcheese.com/p/nvidias-h100-funny-l2-and-tons-of-bandwidth
# Source C is the NVIDIA H100 whitepaper (NVIDIA H100 Tensor Core GPU Architecture)
# Source D is "Inside the NVIDIA Ampere Architecture", a GTC 2020 talk
#   Link: https://isip.piconepress.com/courses/temple/ece_4822/lectures/2021_01_fall/lecture_09a/lecture_09a.pdf