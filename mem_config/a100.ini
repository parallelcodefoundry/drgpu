; This config file its for Tesla A100-PCIE-40GB
[Default]
# For this A100, there are 108 SMs with 2 SMs per TPC, so 54 TPCs.
# Each SM in A100 has 192 kB L1 instead of 128 as in V100
# 1/(108 * 192)
L1_THROUGHPUT_FIX = 0.0000482253
#L1_THROUGHPUT_PEAK = 108 * 192
L1_THROUGHPUT_PEAK = 20736
# For A100, there are 54 TPCs and each one has 2 uTLB (?).
# uTLB_THROUGHPUT_FIX = 1/( 54 * 2 )
# https://images.anandtech.com/doci/14255/TU117_Unofficial_1650_575px.png
uTLB_THROUGHPUT_FIX = 0.00925925925
# For A100, there are  7 GPCs and each one has 2 L1 TLBs (?).
# L1_TLB_THROUGHPUT_FIX = 1 / (7 * 2)
L1_TLB_THROUGHPUT_FIX = 0.07142857142
# For A100, there are 80 L2 slices (Source D) and each read on one slice is 64 bytes.
# Assuming every L2 slice is 512 KB.
# L2_THROUGHPUT_FIX = 1/ ( 80 * 64)
L2_THROUGHPUT_FIX = 0.0001953125
# FB (frame buffer) is the same as DRAM, HBM2
# Peak A100 DRAM bandwidth is 1555 GB/s, and boost clock is 1.41 GHz (Source C)
# FB_THROUGHPUT_FIX = 1 / 1555  * 1.41
FB_THROUGHPUT_FIX = 0.00045608994

compute_capability=80

# Josh: these are unchanged from V100, per Source C
max_avtive_warps_per_SM = 64
warp_size = 32
quadrants_per_SM = 4

# The following two settings are used to limit the number of nodes in decision trees
max_number_of_showed_nodes = 5
max_percentage_of_showed_nodes = 0.99
BYTES_PER_L2_INSTRUCTION = 32
BYTES_PER_L1_INSTRUCTION = 128
conflict_high_threshold = 0.1
low_activewarps_per_activecycle = 16
high_l1_throughput = 0.8
high_l1_hit_rate = 0
high_l1_conflict_rate = 0.2
low_access_per_activate = 4
low_bank_per_access = 10

# @todo
within_load_coalescing_ratio = 1
low_l1_hit_rate = 0.75
high_utlb_miss_rate = .25
high_l2_miss_rate = 0.25
high_l2_bank_conflict_rate = 0.2
high_not_predicated_off_thread_per_inst_executed = 17
max_not_predicated_off_thread_per_inst_executed = 32
low_compress_rate = .05

# Latency part
# Josh: source A
L1_LATENCY_FIX = 34
uTLB_LATENCY_FIX = 1
l1TLB_LATENCY_FIX = 10
# Josh: source A
l2_latency = 275
# Josh: source B
# L2 latencies are bimodally distributed in Ampere due to bifurcation of L2 cache,
# 275 cycles is the overall approximate mean.
fb_latency = 290

# limit how many instructions showed for stall reasons
max_percentage_of_showed_source_code_nodes = 0.95
max_number_of_showed_source_code_nodes = 10

# Josh: key to sources
# Source A is the Dissecting Ampere Architecture With Microbenchmarking talk at GTC 2021
# Source B is the Demystifying the Nvidia Ampere Architecture through Microbenchmarking arXiv
#   paper from 2022
# Source C is the NVIDIA A100 whitepaper (NVIDIA A100 Tensor Core GPU Architecture)
# Source D is "Inside the NVIDIA Ampere Architecture", a GTC 2020 talk
#   Link: https://isip.piconepress.com/courses/temple/ece_4822/lectures/2021_01_fall/lecture_09a/lecture_09a.pdf